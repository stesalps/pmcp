{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stesalps/pmcp/blob/main/Copy_of_colab_models_as_a_service.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWzdSztXWcpg"
      },
      "source": [
        "# Colab AI: List available models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ucchuu5vV3Jp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55d67464-31a6-4f5e-ae92-02527ace248e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['google/gemini-2.0-flash',\n",
              " 'google/gemini-2.0-flash-lite',\n",
              " 'google/gemini-2.5-flash',\n",
              " 'google/gemini-2.5-flash-lite',\n",
              " 'google/gemini-2.5-pro',\n",
              " 'google/gemma-3-12b',\n",
              " 'google/gemma-3-1b',\n",
              " 'google/gemma-3-27b',\n",
              " 'google/gemma-3-4b']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from google.colab import ai\n",
        "ai.list_models()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjfCGEpzDsD9"
      },
      "source": [
        "The model names give you a hint about their capabilities and intended use:\n",
        "\n",
        "Pro: These are the most capable models, ideal for complex reasoning, creative tasks, and detailed analysis.\n",
        "\n",
        "Flash: These models are optimized for high speed and efficiency, making them great for summarization, chat applications, and tasks requiring rapid responses.\n",
        "\n",
        "Gemma: These are lightweight, open-weight models suitable for a variety of text generation tasks and are great for experimentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oypl6kfOX9Jw"
      },
      "source": [
        "# Colab AI: Choose a different model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHO9VzO9AHZP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d985697-1610-4b2d-fdc4-8055e269ee95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of England is **London**.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from google.colab import ai\n",
        "\n",
        "response = ai.generate_text(\"What is the capital of England\", model_name='google/gemini-2.0-flash-lite')\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpUByA5CYQTa"
      },
      "source": [
        "# Colab AI: Simple batch generation example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0rmsI9zYJ-o"
      },
      "outputs": [],
      "source": [
        "# Only text-to-text input/output is supported\n",
        "from google.colab import ai\n",
        "\n",
        "response = ai.generate_text(\"What is the capital of France?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvJPCM4cYawP"
      },
      "source": [
        "# Colab AI: Simple streaming example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BNgxiB6--_5"
      },
      "outputs": [],
      "source": [
        "from google.colab import ai\n",
        "\n",
        "stream = ai.generate_text(\"Tell me a short story.\", stream=True)\n",
        "for text in stream:\n",
        "  print(text, end='')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBJk0a4rY-n1"
      },
      "source": [
        "# Colab AI: Formatted streaming example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpMmpaVClSBV"
      },
      "outputs": [],
      "source": [
        "#code is not necessary for colab.ai, but is useful in fomatting text chunks\n",
        "import sys\n",
        "from google.colab import ai\n",
        "\n",
        "\n",
        "class LineWrapper:\n",
        "    def __init__(self, max_length=80):\n",
        "        self.max_length = max_length\n",
        "        self.current_line_length = 0\n",
        "\n",
        "    def print(self, text_chunk):\n",
        "        i = 0\n",
        "        n = len(text_chunk)\n",
        "        while i < n:\n",
        "            start_index = i\n",
        "            while i < n and text_chunk[i] not in ' \\n': # Find end of word\n",
        "                i += 1\n",
        "            current_word = text_chunk[start_index:i]\n",
        "\n",
        "            delimiter = \"\"\n",
        "            if i < n: # If not end of chunk, we found a delimiter\n",
        "                delimiter = text_chunk[i]\n",
        "                i += 1 # Consume delimiter\n",
        "\n",
        "            if current_word:\n",
        "                needs_leading_space = (self.current_line_length > 0)\n",
        "\n",
        "                # Case 1: Word itself is too long for a line (must be broken)\n",
        "                if len(current_word) > self.max_length:\n",
        "                    if needs_leading_space: # Newline if current line has content\n",
        "                        sys.stdout.write('\\n')\n",
        "                        self.current_line_length = 0\n",
        "                    for char_val in current_word: # Break the long word\n",
        "                        if self.current_line_length >= self.max_length:\n",
        "                            sys.stdout.write('\\n')\n",
        "                            self.current_line_length = 0\n",
        "                        sys.stdout.write(char_val)\n",
        "                        self.current_line_length += 1\n",
        "                # Case 2: Word doesn't fit on current line (print on new line)\n",
        "                elif self.current_line_length + (1 if needs_leading_space else 0) + len(current_word) > self.max_length:\n",
        "                    sys.stdout.write('\\n')\n",
        "                    sys.stdout.write(current_word)\n",
        "                    self.current_line_length = len(current_word)\n",
        "                # Case 3: Word fits on current line\n",
        "                else:\n",
        "                    if needs_leading_space:\n",
        "                        # Define punctuation that should not have a leading space\n",
        "                        # when they form an entire \"word\" (token) following another word.\n",
        "                        no_leading_space_punctuation = {\n",
        "                            \",\", \".\", \";\", \":\", \"!\", \"?\",        # Standard sentence punctuation\n",
        "                            \")\", \"]\", \"}\",                     # Closing brackets\n",
        "                            \"'s\", \"'S\", \"'re\", \"'RE\", \"'ve\", \"'VE\", # Common contractions\n",
        "                            \"'m\", \"'M\", \"'ll\", \"'LL\", \"'d\", \"'D\",\n",
        "                            \"n't\", \"N'T\",\n",
        "                            \"...\", \"‚Ä¶\"                          # Ellipses\n",
        "                        }\n",
        "                        if current_word not in no_leading_space_punctuation:\n",
        "                            sys.stdout.write(' ')\n",
        "                            self.current_line_length += 1\n",
        "                    sys.stdout.write(current_word)\n",
        "                    self.current_line_length += len(current_word)\n",
        "\n",
        "            if delimiter == '\\n':\n",
        "                sys.stdout.write('\\n')\n",
        "                self.current_line_length = 0\n",
        "            elif delimiter == ' ':\n",
        "                # If line is full and a space delimiter arrives, it implies a wrap.\n",
        "                if self.current_line_length >= self.max_length:\n",
        "                    sys.stdout.write('\\n')\n",
        "                    self.current_line_length = 0\n",
        "\n",
        "        sys.stdout.flush()\n",
        "\n",
        "\n",
        "wrapper = LineWrapper()\n",
        "for chunk in ai.generate_text('Give me a long winded description about the evolution of the Roman Empire.', model_name='google/gemini-2.0-flash', stream=True):\n",
        "  wrapper.print(chunk)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6cSTEFk36iPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "# ‚ïë ü§ñ BOX 3: Server Launch and GUI - v7.0.x (Updated for Ollama-powered Box 2 with Chat & Agent Selector) ‚ïë\n",
        "# ‚ïë                                                                                                         ‚ïë\n",
        "# ‚ïü‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ CORE FEATURES ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ï¢\n",
        "# ‚ïë - FastAPI server launch with Uvicorn (Integrates Box 2 or runs proxy)                                   ‚ïë\n",
        "# ‚ïë - Multi-interface support: Jupyter, Gradio                                                          ‚ïë\n",
        "# ‚ïë - Plugin manifests for AI integration (Claude, OpenAI)                                                  ‚ïë\n",
        "# ‚ïë - System integration and monitoring                                                                     ‚ïë\n",
        "# ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
        "\n",
        "print(\"üîß BOX 3: Initializing Server Launch and GUI Systems (Updated for Ollama-powered Box 2 with Chat & Agent Selector)...\")\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import json\n",
        "import asyncio\n",
        "import threading\n",
        "import subprocess\n",
        "import traceback\n",
        "import queue\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Optional, List, Dict, Any\n",
        "\n",
        "print(\"üì• Step 1: Loading configurations from previous boxes (Updated Paths)...\")\n",
        "\n",
        "# --- Load Box 1 and Box 2 configurations ---\n",
        "try:\n",
        "    # Standardized config directory path from updated Box 1\n",
        "    config_dir = Path(\"/content/drive/MyDrive/UnifiedManusSystem/config\")\n",
        "    # Fallback for local runs\n",
        "    if not config_dir.exists():\n",
        "        config_dir = Path(\"./UnifiedManusSystem/config\")\n",
        "\n",
        "    if not config_dir.exists():\n",
        "        raise FileNotFoundError(\"Config directory not found\")\n",
        "\n",
        "    # Load Box 1 config\n",
        "    box1_config_file = config_dir / \"box1_exports.json\"\n",
        "    if box1_config_file.exists():\n",
        "        with open(box1_config_file, \"r\") as f:\n",
        "            box1_config = json.load(f)\n",
        "        print(\"‚úÖ Box 1 configuration loaded\")\n",
        "    else:\n",
        "        raise FileNotFoundError(\"Box 1 config not found\")\n",
        "\n",
        "    # Load Box 2 config (now includes Ollama details)\n",
        "    box2_config_file = config_dir / \"box2_exports.json\"\n",
        "    if box2_config_file.exists():\n",
        "        with open(box2_config_file, \"r\") as f:\n",
        "            box2_config = json.load(f)\n",
        "        print(\"‚úÖ Box 2 configuration loaded\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Box 2 config not found, will use defaults/fallbacks\")\n",
        "        box2_config = {\"tools_registered\": [], \"agent_session\": \"unknown\", \"api_endpoints\": [], \"ollama_model\": \"unknown\"}\n",
        "\n",
        "    # Extract configuration\n",
        "    BASE_DIR = Path(box1_config[\"BASE_DIR\"])\n",
        "    WORKSPACE_DIR = Path(box1_config[\"WORKSPACE_DIR\"])\n",
        "    LOG_FILE = Path(box1_config[\"LOG_FILE\"])\n",
        "    public_url = box1_config[\"public_url\"]\n",
        "    dashboard_url = box1_config[\"dashboard_url\"]\n",
        "    IS_COLAB = box1_config[\"IS_COLAB\"]\n",
        "\n",
        "    tools_available = box2_config.get(\"tools_registered\", [])\n",
        "    agent_session = box2_config.get(\"agent_session\", \"unknown\")\n",
        "    box2_api_endpoints = box2_config.get(\"api_endpoints\", [])\n",
        "    ollama_model = box2_config.get(\"ollama_model\", \"unknown\")\n",
        "\n",
        "    print(f\"üìÅ Base Directory: {BASE_DIR}\")\n",
        "    print(f\"üåç Public URL: {public_url}\")\n",
        "    print(f\"ü§ñ Agent Session: {agent_session}\")\n",
        "    print(f\"ü¶ô Ollama Model: {ollama_model}\")\n",
        "    print(f\"üõ†Ô∏è Tools Available: {len(tools_available)}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading configurations: {e}\")\n",
        "    print(\"üîÑ Using fallback configuration...\")\n",
        "    BASE_DIR = Path(\"/content/drive/MyDrive/UnifiedManusSystem\")\n",
        "    WORKSPACE_DIR = BASE_DIR / \"workspace\"\n",
        "    LOG_FILE = BASE_DIR / \"logs\" / \"manus_log.json\"\n",
        "    public_url = \"http://localhost:8000\"\n",
        "    dashboard_url = \"http://localhost:5000\"\n",
        "    IS_COLAB = True\n",
        "    tools_available = []\n",
        "    agent_session = \"unknown\"\n",
        "    box2_api_endpoints = []\n",
        "    ollama_model = \"unknown\"\n",
        "\n",
        "# Ensure we're in the right directory\n",
        "if BASE_DIR.exists():\n",
        "    os.chdir(BASE_DIR)\n",
        "\n",
        "print(\"üì¶ Step 2: Importing required modules for Box 3...\")\n",
        "\n",
        "# Apply nest_asyncio (Important for Jupyter environments)\n",
        "try:\n",
        "    import nest_asyncio\n",
        "    nest_asyncio.apply()\n",
        "    print(\"üîÑ nest_asyncio applied\")\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è nest_asyncio not found (might be needed in Jupyter)\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Error applying nest_asyncio: {e}\")\n",
        "\n",
        "# Core web framework\n",
        "import uvicorn\n",
        "from fastapi import FastAPI, Request, BackgroundTasks\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from fastapi.staticfiles import StaticFiles\n",
        "from fastapi.responses import FileResponse, JSONResponse\n",
        "import requests # For proxying calls to Box 2 if needed\n",
        "\n",
        "# GUI frameworks\n",
        "GRADIO_AVAILABLE = False\n",
        "JUPYTER_AVAILABLE = False\n",
        "try:\n",
        "    import gradio as gr\n",
        "    GRADIO_AVAILABLE = True\n",
        "    print(\"‚úÖ Gradio available\")\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è Gradio not available\")\n",
        "\n",
        "try:\n",
        "    from IPython.display import display, HTML, clear_output\n",
        "    import ipywidgets as widgets\n",
        "    JUPYTER_AVAILABLE = True\n",
        "    print(\"‚úÖ Jupyter widgets available\")\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è Jupyter widgets not available\")\n",
        "\n",
        "print(\"‚úÖ All Box 3 modules imported successfully\")\n",
        "\n",
        "print(\"üîÑ Step 3: Re-establishing connection to Box 2 components...\")\n",
        "\n",
        "# --- Determine if Box 2 is running ---\n",
        "# Check if Box 2's app and agent are available in the current session (e.g., if this is a unified script)\n",
        "box2_running_in_session = 'app' in globals() and 'agent' in globals() and 'TOOL_REGISTRY' in globals()\n",
        "box2_api_accessible = False\n",
        "box2_api_url = \"http://localhost:8000\" # Default assumption for internal calls\n",
        "\n",
        "if box2_running_in_session:\n",
        "    print(\"‚úÖ Box 2 components found in current session (unified script mode)\")\n",
        "    box2_running = True\n",
        "    # Use the in-session components\n",
        "    try:\n",
        "        from __main__ import app as box2_app, agent as box2_agent, TOOL_REGISTRY as box2_tool_registry\n",
        "        print(\"üîó Linked to in-session Box 2 components\")\n",
        "    except ImportError:\n",
        "        print(\"‚ö†Ô∏è Could not import Box 2 components directly, using global references if available\")\n",
        "        # They are already in globals if the check passed\n",
        "        box2_app = globals().get('app')\n",
        "        box2_agent = globals().get('agent')\n",
        "        box2_tool_registry = globals().get('TOOL_REGISTRY')\n",
        "else:\n",
        "    # Check if Box 2 server is running externally\n",
        "    try:\n",
        "        response = requests.get(f\"{box2_api_url}/health\", timeout=5)\n",
        "        if response.status_code == 200:\n",
        "            print(\"‚úÖ Box 2 server is accessible externally\")\n",
        "            box2_running = True\n",
        "            box2_api_accessible = True\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Box 2 server health check failed (Status: {response.status_code})\")\n",
        "            box2_running = False\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"‚ö†Ô∏è Could not connect to Box 2 server at {box2_api_url}: {e}\")\n",
        "        box2_running = False\n",
        "\n",
        "if not box2_running:\n",
        "    print(\"‚ö†Ô∏è Box 2 components not found/runnable. GUI will use fallback methods or fail gracefully.\")\n",
        "\n",
        "\n",
        "print(\"üìÑ Step 4: Creating plugin manifest files...\")\n",
        "\n",
        "def create_plugin_manifests():\n",
        "    \"\"\"Create plugin manifest files for AI integration\"\"\"\n",
        "    print(\"üìÑ Creating plugin manifest files...\")\n",
        "    site_dir = BASE_DIR / \"site\"\n",
        "    site_dir.mkdir(exist_ok=True)\n",
        "    static_dir = site_dir / \"static\"\n",
        "    static_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    # AI Plugin manifest (OpenAI/Claude compatible structure)\n",
        "    ai_plugin_manifest = {\n",
        "        \"schema_version\": \"v1\",\n",
        "        \"name_for_human\": \"Unified Manus MCP\",\n",
        "        \"name_for_model\": \"unified_manus\",\n",
        "        \"description_for_human\": \"Multi-agent coding assistant with comprehensive tool support, powered by Ollama LLM.\",\n",
        "        \"description_for_model\": f\"A unified agent system with file operations, Python execution, package management, and multi-role thinking capabilities via the 'action_agent' tool, using the {ollama_model} model. Also supports direct chat via /mcp/chat.\",\n",
        "        \"auth\": {\"type\": \"none\"},\n",
        "        \"api\": {\"type\": \"openapi\", \"url\": f\"{public_url}/openapi.json\"}, # Points to Box 2's OpenAPI spec\n",
        "        \"logo_url\": f\"{public_url}/site/static/logo.png\", # Placeholder\n",
        "        \"contact_email\": \"support@example.com\",\n",
        "        \"legal_info_url\": f\"{public_url}/site/legal.html\" # Placeholder\n",
        "    }\n",
        "    try:\n",
        "        with open(site_dir / \"ai-plugin.json\", \"w\") as f:\n",
        "            json.dump(ai_plugin_manifest, f, indent=2)\n",
        "        print(\"‚úÖ AI Plugin manifest created\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to create AI Plugin manifest: {e}\")\n",
        "\n",
        "    # Claude-compatible manifest (YAML)\n",
        "    claude_manifest = {\n",
        "        \"name\": \"unified_manus\",\n",
        "        \"description\": f\"Multi-agent coding assistant with comprehensive tool support, including an 'action_agent' for complex tasks and a '/mcp/chat' endpoint for direct conversation, powered by Ollama ({ollama_model}).\",\n",
        "        \"version\": \"7.0.x\",\n",
        "        \"endpoints\": {\n",
        "            \"tool_call\": f\"{public_url}/mcp/tools/call\",\n",
        "            \"tool_list\": f\"{public_url}/mcp/tools/list\",\n",
        "            \"chat\": f\"{public_url}/mcp/chat\" # Include the new chat endpoint\n",
        "        },\n",
        "        \"capabilities\": [\"file_operations\", \"python_execution\", \"package_management\", \"agent_thinking\", \"memory_management\", \"chat\"]\n",
        "    }\n",
        "    try:\n",
        "        import yaml # Should be available as installed by updated Box 1\n",
        "        with open(site_dir / \"claude.yaml\", \"w\") as f:\n",
        "            yaml.dump(claude_manifest, f, default_flow_style=False)\n",
        "        print(\"‚úÖ Claude manifest (YAML) created\")\n",
        "    except ImportError:\n",
        "        print(\"‚ö†Ô∏è PyYAML not found, skipping Claude manifest YAML creation.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to create Claude manifest: {e}\")\n",
        "\n",
        "    # Simple index.html for /site/\n",
        "    index_content = f\"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>Unified Manus System (Ollama)</title>\n",
        "</head>\n",
        "<body>\n",
        "    <h1>ü§ñ Unified Manus MCP System v7.0.x (Ollama-powered)</h1>\n",
        "    <p>Multi-Agent Coding Assistant</p>\n",
        "    <ul>\n",
        "        <li><a href=\"/docs\">API Documentation (FastAPI)</a></li>\n",
        "        <li><a href=\"/redoc\">API Documentation (ReDoc)</a></li>\n",
        "        <li>Agent Session: {agent_session}</li>\n",
        "        <li>Ollama Model: {ollama_model}</li>\n",
        "        <li>Tools Available: {len(tools_available)}</li>\n",
        "        <li>Public URL: <a href=\"{public_url}\">{public_url}</a></li>\n",
        "    </ul>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "    try:\n",
        "        with open(site_dir / \"index.html\", \"w\") as f:\n",
        "            f.write(index_content)\n",
        "        print(\"‚úÖ Basic site index.html created\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to create site index.html: {e}\")\n",
        "\n",
        "create_plugin_manifests()\n",
        "\n",
        "\n",
        "print(\"üé® Step 5: Setting up Gradio and Jupyter interfaces...\")\n",
        "\n",
        "# --- Gradio Interface ---\n",
        "def setup_gradio_interface():\n",
        "    \"\"\"Setup Gradio interface\"\"\"\n",
        "    if not GRADIO_AVAILABLE:\n",
        "        print(\"‚ö†Ô∏è Gradio not available, skipping Gradio interface\")\n",
        "        return None\n",
        "\n",
        "    print(\"üé® Setting up Gradio interface...\")\n",
        "\n",
        "    def run_agent_task(task, context):\n",
        "        \"\"\"Run a task through the agent\"\"\"\n",
        "        try:\n",
        "            if box2_running_in_session:\n",
        "                # Direct call to in-session agent\n",
        "                result_dict = box2_agent.run_task(task, context)\n",
        "                if result_dict.get(\"status\") == \"success\":\n",
        "                     return result_dict.get(\"final_output\", \"No final output found.\")\n",
        "                else:\n",
        "                     return f\"‚ùå Agent Error: {result_dict.get('message', 'Unknown error')}\"\n",
        "            elif box2_api_accessible:\n",
        "                 payload = {\"task\": task, \"context\": context}\n",
        "                 response = requests.post(f\"{box2_api_url}/mcp/agent/action\", json=payload, timeout=120)\n",
        "                 if response.status_code == 200:\n",
        "                     result_dict = response.json()\n",
        "                     if result_dict.get(\"status\") == \"success\":\n",
        "                          return result_dict.get(\"final_output\", \"No final output found in API response.\")\n",
        "                     else:\n",
        "                          return f\"‚ùå Agent API Error: {result_dict.get('message', 'Unknown error from API')}\"\n",
        "                 else:\n",
        "                     return f\"‚ùå API Error ({response.status_code}): {response.text}\"\n",
        "            else:\n",
        "                 return \"‚ùå Box 2 (Agent Core) is not available.\"\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Failed to run agent task: {str(e)}\"\n",
        "\n",
        "    def list_tools():\n",
        "        \"\"\"List available tools\"\"\"\n",
        "        try:\n",
        "             if box2_running_in_session:\n",
        "                 from __main__ import TOOL_REGISTRY\n",
        "                 tools_text = f\"üõ†Ô∏è Available Tools ({len(TOOL_REGISTRY)}):\\n\"\n",
        "                 for name, func in TOOL_REGISTRY.items():\n",
        "                     desc = func.__doc__.split('\\n')[0] if func.__doc__ else \"No description\"\n",
        "                     tools_text += f\"‚Ä¢ {name}: {desc}\\n\"\n",
        "                 return tools_text\n",
        "             elif box2_api_accessible:\n",
        "                 response = requests.get(f\"{box2_api_url}/mcp/tools/list\", timeout=10)\n",
        "                 if response.status_code == 200:\n",
        "                     result = response.json()\n",
        "                     tools_text = f\"üõ†Ô∏è Available Tools ({result['count']}):\\n\"\n",
        "                     for tool in result['tools']:\n",
        "                         tools_text += f\"‚Ä¢ {tool['name']}: {tool['description']}\\n\"\n",
        "                     return tools_text\n",
        "                 else:\n",
        "                     return f\"‚ùå API Error ({response.status_code}): {response.text}\"\n",
        "             else:\n",
        "                  return \"‚ùå Box 2 (Tool Registry) is not available.\"\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Failed to fetch tools: {str(e)}\"\n",
        "\n",
        "    def execute_tool(tool_name, tool_input):\n",
        "        \"\"\"Execute a specific tool\"\"\"\n",
        "        try:\n",
        "            if isinstance(tool_input, str) and tool_input.strip():\n",
        "                try:\n",
        "                    input_data = json.loads(tool_input)\n",
        "                except json.JSONDecodeError:\n",
        "                    input_data = {\"content\": tool_input}\n",
        "            else:\n",
        "                input_data = {}\n",
        "\n",
        "            if box2_running_in_session:\n",
        "                from __main__ import TOOL_REGISTRY\n",
        "                if tool_name in TOOL_REGISTRY:\n",
        "                    result = TOOL_REGISTRY[tool_name](**input_data)\n",
        "                    if isinstance(result, dict):\n",
        "                        return json.dumps(result, indent=2)\n",
        "                    return str(result)\n",
        "                else:\n",
        "                    return f\"‚ùå Tool '{tool_name}' not found.\"\n",
        "            elif box2_api_accessible:\n",
        "                payload = {\"tool_name\": tool_name, \"tool_input\": input_data}\n",
        "                response = requests.post(f\"{box2_api_url}/mcp/tools/call\", json=payload, timeout=60)\n",
        "                if response.status_code == 200:\n",
        "                    res_data = response.json()\n",
        "                    result_content = res_data.get(\"result\", \"No result field\")\n",
        "                    if isinstance(result_content, dict):\n",
        "                        return json.dumps(result_content, indent=2)\n",
        "                    return str(result_content)\n",
        "                else:\n",
        "                     return f\"‚ùå API Error ({response.status_code}): {response.text}\"\n",
        "            else:\n",
        "                 return \"‚ùå Box 2 (Tool Execution) is not available.\"\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Failed to execute tool: {str(e)}\"\n",
        "\n",
        "    # --- New Chat Functionality for Gradio ---\n",
        "    def chat_with_model(message_history):\n",
        "        \"\"\"\n",
        "        Chat with the model via the new /mcp/chat endpoint.\n",
        "        Gradio's ChatInterface passes message_history as a list of [user_msg, bot_msg, user_msg, ...]\n",
        "        We need to convert it to the format expected by /mcp/chat: [{\"role\": \"...\", \"content\": \"...\"}]\n",
        "        \"\"\"\n",
        "        # Convert Gradio history to Ollama format\n",
        "        ollama_messages = []\n",
        "        for i, msg in enumerate(message_history):\n",
        "            if i % 2 == 0: # User message\n",
        "                ollama_messages.append({\"role\": \"user\", \"content\": msg})\n",
        "            else: # Bot message\n",
        "                ollama_messages.append({\"role\": \"assistant\", \"content\": msg})\n",
        "\n",
        "        if box2_running_in_session and box2_api_accessible:\n",
        "            # Prefer direct API call for streaming\n",
        "            try:\n",
        "                payload = {\"messages\": ollama_messages}\n",
        "                # Note: Streaming responses from external APIs to Gradio chatbot can be complex.\n",
        "                # A simpler approach is to make a non-streaming call.\n",
        "                response = requests.post(f\"{box2_api_url}/mcp/chat\", json=payload, timeout=120)\n",
        "                if response.status_code == 200:\n",
        "                    # The /mcp/chat endpoint returns a stream. If we get a non-stream response here,\n",
        "                    # it might be an aggregated final message or an error.\n",
        "                    # Let's try to parse JSON. If it fails, return raw text.\n",
        "                    try:\n",
        "                        data = response.json()\n",
        "                        # Check for common fields in the final aggregated response\n",
        "                        if \"message\" in data:\n",
        "                            # If it's the final message structure from the streaming endpoint\n",
        "                            return data.get(\"message\", {}).get(\"content\", \"Received message, content unclear (format 1).\")\n",
        "                        elif \"content\" in data:\n",
        "                             # If it's a simple content response\n",
        "                             return data[\"content\"]\n",
        "                        elif \"final_output\" in data: # Maybe the action_agent format was used somehow\n",
        "                            return data[\"final_output\"]\n",
        "                        else:\n",
        "                            # Return the whole JSON if structure is unknown\n",
        "                            return f\"Received JSON, structure unclear: {data}\"\n",
        "                    except json.JSONDecodeError:\n",
        "                        # If response isn't JSON, return text content\n",
        "                        return response.text or \"Received response, but it was empty.\"\n",
        "                else:\n",
        "                    return f\"‚ùå Chat API Error ({response.status_code}): {response.text}\"\n",
        "            except Exception as e:\n",
        "                 return f\"‚ùå Chat API Call Failed: {str(e)}\"\n",
        "        elif box2_running_in_session:\n",
        "            # If only running in session, we'd need a way to call the Ollama chat function directly\n",
        "            # and handle streaming. This is more complex in Gradio without async support in this context.\n",
        "            # Fallback: Use a simple non-streaming direct call (if such a function exists or is adapted).\n",
        "            # For now, indicate it's not fully supported via direct call in this GUI setup.\n",
        "            return \"‚ö†Ô∏è Direct chat not fully implemented in this GUI mode. Use API or Jupyter GUI for full chat.\"\n",
        "        else:\n",
        "             return \"‚ùå Box 2 Chat API is not accessible.\"\n",
        "\n",
        "    with gr.Blocks(title=\"Unified Manus MCP System (Ollama)\", theme=gr.themes.Default()) as demo:\n",
        "        gr.Markdown(\"# ü§ñ Unified Manus MCP System v7.0.x (Ollama-powered)\")\n",
        "        gr.Markdown(\"Multi-Agent Coding Assistant with LLM Capabilities\")\n",
        "\n",
        "        with gr.Tab(\"Agent Tasks\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    task_input = gr.Textbox(label=\"Task\", placeholder=\"Enter a task for the agent...\")\n",
        "                    context_input = gr.Textbox(label=\"Context (optional)\", placeholder=\"Additional context...\", lines=3)\n",
        "                    run_btn = gr.Button(\"Run Task with Action Agent\", variant=\"primary\")\n",
        "                with gr.Column():\n",
        "                    task_output = gr.Textbox(label=\"Agent Output\", lines=20, max_lines=30, show_copy_button=True)\n",
        "            run_btn.click(fn=run_agent_task, inputs=[task_input, context_input], outputs=task_output)\n",
        "\n",
        "        with gr.Tab(\"Tool Execution\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    tool_name_input = gr.Dropdown(label=\"Tool Name\", choices=tools_available if tools_available else [], allow_custom_value=True)\n",
        "                    tool_input_input = gr.Textbox(label=\"Tool Input (JSON)\", placeholder='{\"file_path\": \"test.txt\", \"content\": \"Hello\"}', lines=5)\n",
        "                    exec_tool_btn = gr.Button(\"Execute Tool\", variant=\"secondary\")\n",
        "                with gr.Column():\n",
        "                    tool_output = gr.Textbox(label=\"Tool Output\", lines=15, max_lines=20, show_copy_button=True)\n",
        "            exec_tool_btn.click(fn=execute_tool, inputs=[tool_name_input, tool_input_input], outputs=tool_output)\n",
        "\n",
        "        with gr.Tab(\"Direct Chat (via /mcp/chat)\"):\n",
        "             # Gradio's ChatInterface is a convenient way to handle chat\n",
        "             chatbot = gr.Chatbot(label=\"Conversation\")\n",
        "             msg = gr.Textbox(label=\"Your Message\", placeholder=\"Type your message here...\")\n",
        "             clear_chat = gr.Button(\"Clear Chat\")\n",
        "\n",
        "             def respond(message, chat_history):\n",
        "                 # Append user message to history\n",
        "                 chat_history.append((message, None))\n",
        "                 # Get response from chat function\n",
        "                 bot_message = chat_with_model([item for sublist in chat_history for item in sublist if item is not None])\n",
        "                 # Update the last entry in history with the bot's response\n",
        "                 chat_history[-1] = (message, bot_message)\n",
        "                 return \"\", chat_history\n",
        "\n",
        "             msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
        "             clear_chat.click(fn=lambda: ([], []), inputs=[], outputs=[chatbot, msg], queue=False)\n",
        "\n",
        "\n",
        "        with gr.Tab(\"System Info\"):\n",
        "            with gr.Row():\n",
        "                tools_btn = gr.Button(\"Refresh Tool List\")\n",
        "                tools_output = gr.Textbox(label=\"Available Tools\", lines=15, max_lines=20)\n",
        "                tools_btn.click(fn=list_tools, outputs=tools_output)\n",
        "\n",
        "            info_text = (\n",
        "                f\"**System Information:**\\n\"\n",
        "                f\"- Public API URL: {public_url}\\n\"\n",
        "                f\"- Dashboard URL: {dashboard_url}\\n\"\n",
        "                f\"- Agent Session: {agent_session}\\n\"\n",
        "                f\"- Ollama Model: {ollama_model}\\n\"\n",
        "                f\"- Base Directory: {BASE_DIR}\\n\"\n",
        "                f\"- Workspace Directory: {WORKSPACE_DIR}\\n\"\n",
        "                f\"- Tools Available: {len(tools_available)}\\n\"\n",
        "                f\"- Box 2 Status: {'Integrated/Running' if box2_running else 'Not Accessible'}\"\n",
        "            )\n",
        "            gr.Markdown(info_text)\n",
        "\n",
        "    return demo\n",
        "\n",
        "# --- Jupyter Interface ---\n",
        "def setup_jupyter_interface():\n",
        "    \"\"\"Setup Jupyter widget interface\"\"\"\n",
        "    if not JUPYTER_AVAILABLE:\n",
        "        print(\"‚ö†Ô∏è Jupyter widgets not available, skipping Jupyter interface\")\n",
        "        return None\n",
        "\n",
        "    print(\"üìì Setting up Jupyter interface...\")\n",
        "\n",
        "    # --- Jupyter UI Logic ---\n",
        "    def create_jupyter_ui():\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        # --- UI Elements ---\n",
        "        task_input = widgets.Text(\n",
        "            value='',\n",
        "            placeholder='Enter a task for the agent (e.g., Write a Python script to calculate Fibonacci numbers)',\n",
        "            description='Task:',\n",
        "            disabled=False,\n",
        "            layout=widgets.Layout(width='100%')\n",
        "        )\n",
        "\n",
        "        context_input = widgets.Textarea(\n",
        "            value='',\n",
        "            placeholder='Optional context for the task',\n",
        "            description='Context:',\n",
        "            disabled=False,\n",
        "            layout=widgets.Layout(width='100%', height='100px')\n",
        "        )\n",
        "\n",
        "        run_button = widgets.Button(\n",
        "            description=\"Run Task with Action Agent\",\n",
        "            button_style='success',\n",
        "            tooltip='Execute the task using the internal Ollama-powered ManusAgent',\n",
        "            icon='play'\n",
        "        )\n",
        "\n",
        "        tool_name_dropdown = widgets.Dropdown(\n",
        "            options=tools_available if tools_available else ['No tools loaded'],\n",
        "            value=tools_available[0] if tools_available else 'No tools loaded',\n",
        "            description='Tool:',\n",
        "            disabled=not tools_available,\n",
        "        )\n",
        "\n",
        "        tool_input_textarea = widgets.Textarea(\n",
        "            value='{}',\n",
        "            placeholder='Enter tool input as JSON (e.g., {\"file_path\": \"test.txt\", \"content\": \"Hello\"})',\n",
        "            description='Input (JSON):',\n",
        "            disabled=False,\n",
        "            layout=widgets.Layout(width='100%', height='100px')\n",
        "        )\n",
        "\n",
        "        tool_run_button = widgets.Button(\n",
        "            description=\"Execute Tool\",\n",
        "            button_style='info',\n",
        "            tooltip='Run the selected tool with the provided input',\n",
        "            icon='cogs'\n",
        "        )\n",
        "\n",
        "        clear_button = widgets.Button(\n",
        "            description=\"Clear Output\",\n",
        "            button_style='warning',\n",
        "            tooltip='Clear the output areas below',\n",
        "            icon='eraser'\n",
        "        )\n",
        "\n",
        "        output_area = widgets.Output(\n",
        "            layout=widgets.Layout(height='400px', border='1px solid black', overflow='auto', padding='10px')\n",
        "        )\n",
        "\n",
        "        # --- Chat Elements ---\n",
        "        chat_history_area = widgets.Output(\n",
        "             layout=widgets.Layout(height='300px', border='1px solid blue', overflow='auto', padding='10px', background_color='#f0f8ff')\n",
        "        )\n",
        "        chat_input = widgets.Textarea(\n",
        "            value='',\n",
        "            placeholder='Type your message here for direct chat...',\n",
        "            description='Chat:',\n",
        "            disabled=False,\n",
        "            layout=widgets.Layout(width='100%', height='80px')\n",
        "        )\n",
        "        # --- Agent Selection Dropdown for Chat ---\n",
        "        chat_agent_selector = widgets.Dropdown(\n",
        "            options=[\n",
        "                (\"Direct Chat with Ollama Model\", \"direct_chat\"),\n",
        "                (\"Chat via Action Agent Team\", \"action_agent\")\n",
        "            ],\n",
        "            value=\"action_agent\", # Default to action agent\n",
        "            description=\"Chat Agent:\",\n",
        "            disabled=False,\n",
        "        )\n",
        "        chat_send_button = widgets.Button(\n",
        "            description=\"Send Message\",\n",
        "            button_style='primary',\n",
        "            tooltip='Send message to the selected agent',\n",
        "            icon='paper-plane'\n",
        "        )\n",
        "        chat_clear_button = widgets.Button(\n",
        "            description=\"Clear Chat\",\n",
        "            button_style='warning',\n",
        "            tooltip='Clear the chat history',\n",
        "            icon='trash'\n",
        "        )\n",
        "\n",
        "        # --- Event Handlers ---\n",
        "        def on_run_task(b):\n",
        "            task = task_input.value\n",
        "            context = context_input.value\n",
        "            if not task:\n",
        "                with output_area:\n",
        "                    print(\"‚ö†Ô∏è Please enter a task.\")\n",
        "                return\n",
        "\n",
        "            with output_area:\n",
        "                clear_output(wait=True)\n",
        "                print(f\"üöÄ Running task: '{task}'\")\n",
        "                if context:\n",
        "                    print(f\"   Context: '{context}'\")\n",
        "                print(\"-\" * 20)\n",
        "\n",
        "                try:\n",
        "                    if box2_running_in_session:\n",
        "                        result_dict = box2_agent.run_task(task, context)\n",
        "                        if result_dict.get(\"status\") == \"success\":\n",
        "                             print(\"‚úÖ Task completed!\")\n",
        "                             print(\"\\nüìù Plan:\")\n",
        "                             print(result_dict.get(\"plan\", \"N/A\"))\n",
        "                             print(\"\\nüíª Generated Code:\")\n",
        "                             print(result_dict.get(\"code\", \"N/A\"))\n",
        "                             print(\"\\nüîç Review:\")\n",
        "                             print(result_dict.get(\"review\", \"N/A\"))\n",
        "                             print(\"\\n--- Final Output ---\")\n",
        "                             print(result_dict.get(\"final_output\", \"N/A\"))\n",
        "                        else:\n",
        "                             print(f\"‚ùå Agent Error: {result_dict.get('message', 'Unknown error')}\")\n",
        "                    elif box2_api_accessible:\n",
        "                         payload = {\"task\": task, \"context\": context}\n",
        "                         response = requests.post(f\"{box2_api_url}/mcp/agent/action\", json=payload, timeout=120)\n",
        "                         if response.status_code == 200:\n",
        "                             result_dict = response.json()\n",
        "                             if result_dict.get(\"status\") == \"success\":\n",
        "                                  print(\"‚úÖ Task completed!\")\n",
        "                                  print(\"\\nüìù Plan:\")\n",
        "                                  print(result_dict.get(\"plan\", \"N/A\"))\n",
        "                                  print(\"\\nüíª Generated Code:\")\n",
        "                                  print(result_dict.get(\"code\", \"N/A\"))\n",
        "                                  print(\"\\nüîç Review:\")\n",
        "                                  print(result_dict.get(\"review\", \"N/A\"))\n",
        "                                  print(\"\\n--- Final Output ---\")\n",
        "                                  print(result_dict.get(\"final_output\", \"N/A\"))\n",
        "                             else:\n",
        "                                  print(f\"‚ùå Agent API Error: {result_dict.get('message', 'Unknown error from API')}\")\n",
        "                         else:\n",
        "                             print(f\"‚ùå API Error ({response.status_code}): {response.text}\")\n",
        "                    else:\n",
        "                         print(\"‚ùå Box 2 (Agent Core) is not accessible.\")\n",
        "                except Exception as e:\n",
        "                    print(f\"üí• ERROR running task: {e}\")\n",
        "                    traceback.print_exc()\n",
        "\n",
        "        def on_execute_tool(b):\n",
        "            tool_name = tool_name_dropdown.value\n",
        "            tool_input_str = tool_input_textarea.value\n",
        "\n",
        "            if tool_name == 'No tools loaded':\n",
        "                 with output_area:\n",
        "                     print(\"‚ö†Ô∏è No tools are available to execute.\")\n",
        "                 return\n",
        "\n",
        "            try:\n",
        "                if tool_input_str.strip():\n",
        "                    tool_input_data = json.loads(tool_input_str)\n",
        "                else:\n",
        "                    tool_input_data = {}\n",
        "            except json.JSONDecodeError as e:\n",
        "                 with output_area:\n",
        "                     print(f\"‚ùå Invalid JSON input for tool: {e}\")\n",
        "                 return\n",
        "\n",
        "            with output_area:\n",
        "                clear_output(wait=True)\n",
        "                print(f\"üîß Executing tool: '{tool_name}'\")\n",
        "                print(f\"   Input: {json.dumps(tool_input_data, indent=2)}\")\n",
        "                print(\"-\" * 20)\n",
        "\n",
        "                try:\n",
        "                    if box2_running_in_session:\n",
        "                        if tool_name in box2_tool_registry:\n",
        "                            result = box2_tool_registry[tool_name](**tool_input_data)\n",
        "                            print(\"‚úÖ Tool executed successfully!\")\n",
        "                            print(json.dumps(result, indent=2) if isinstance(result, dict) else str(result))\n",
        "                        else:\n",
        "                            print(f\"‚ùå Tool '{tool_name}' not found in registry.\")\n",
        "                    elif box2_api_accessible:\n",
        "                        payload = {\"tool_name\": tool_name, \"tool_input\": tool_input_data}\n",
        "                        response = requests.post(f\"{box2_api_url}/mcp/tools/call\", json=payload, timeout=60)\n",
        "                        if response.status_code == 200:\n",
        "                            res_data = response.json()\n",
        "                            result_content = res_data.get(\"result\", \"No result field\")\n",
        "                            print(\"‚úÖ Tool executed successfully!\")\n",
        "                            print(json.dumps(result_content, indent=2) if isinstance(result_content, dict) else str(result_content))\n",
        "                        else:\n",
        "                             print(f\"‚ùå API Error ({response.status_code}): {response.text}\")\n",
        "                    else:\n",
        "                         print(\"‚ùå Box 2 (Tool Execution) is not accessible.\")\n",
        "                except Exception as e:\n",
        "                    print(f\"üí• ERROR executing tool: {e}\")\n",
        "                    traceback.print_exc()\n",
        "\n",
        "        def on_clear(b):\n",
        "            output_area.clear_output()\n",
        "\n",
        "        # --- Chat Event Handlers (Revised) ---\n",
        "        def on_chat_send(b):\n",
        "            user_message = chat_input.value.strip()\n",
        "            selected_agent = chat_agent_selector.value\n",
        "            if not user_message:\n",
        "                with chat_history_area:\n",
        "                    print(\"‚ö†Ô∏è Please enter a message to send.\")\n",
        "                return\n",
        "\n",
        "            with chat_history_area:\n",
        "                print(f\"[You]: {user_message}\")\n",
        "\n",
        "            try:\n",
        "                if selected_agent == \"direct_chat\" and box2_api_accessible:\n",
        "                    # Call the new /mcp/chat endpoint for direct conversation\n",
        "                    messages = [{\"role\": \"user\", \"content\": user_message}]\n",
        "                    payload = {\"messages\": messages}\n",
        "                    response = requests.post(f\"{box2_api_url}/mcp/chat\", json=payload, timeout=120, stream=True)\n",
        "                    if response.status_code == 200:\n",
        "                        full_response = \"\"\n",
        "                        with chat_history_area:\n",
        "                            print(f\"[Assistant ({ollama_model})]: \", end=\"\", flush=True)\n",
        "                        for line in response.iter_lines():\n",
        "                            if line:\n",
        "                                try:\n",
        "                                    chunk_data = json.loads(line)\n",
        "                                    if chunk_data.get(\"event\") == \"message\":\n",
        "                                        content = chunk_data.get(\"data\", {}).get(\"content\", \"\")\n",
        "                                        full_response += content\n",
        "                                        with chat_history_area:\n",
        "                                            print(content, end=\"\", flush=True)\n",
        "                                    elif chunk_data.get(\"event\") == \"end\":\n",
        "                                        break\n",
        "                                    elif chunk_data.get(\"event\") == \"error\":\n",
        "                                        error_msg = chunk_data.get(\"data\", {}).get(\"error\", \"Unknown stream error\")\n",
        "                                        with chat_history_area:\n",
        "                                            print(f\"\\n‚ùå Stream Error: {error_msg}\")\n",
        "                                        break\n",
        "                                except json.JSONDecodeError:\n",
        "                                    with chat_history_area:\n",
        "                                        print(f\"\\n‚ö†Ô∏è Malformed stream data received.\")\n",
        "                        with chat_history_area:\n",
        "                             print() # Newline after streaming\n",
        "                        chat_input.value = ''\n",
        "                    else:\n",
        "                         with chat_history_area:\n",
        "                             print(f\"\\n‚ùå Chat API Error ({response.status_code}): {response.text}\")\n",
        "                         chat_input.value = ''\n",
        "\n",
        "                elif selected_agent == \"action_agent\":\n",
        "                    # Send message to the Action Agent Team\n",
        "                    # Attempt to capture context from output_area\n",
        "                    # Note: Directly reading from widgets.Output is tricky.\n",
        "                    # We pass a generic context string. A more advanced version\n",
        "                    # might store the last output in a variable.\n",
        "                    context_from_output = \"See previous output in the 'Output' section above for context.\"\n",
        "\n",
        "                    with chat_history_area:\n",
        "                        print(f\"[Assistant (Action Agent Team)]: Thinking... \", end=\"\", flush=True)\n",
        "\n",
        "                    try:\n",
        "                        if box2_running_in_session:\n",
        "                            # Direct call to in-session agent\n",
        "                            result_dict = box2_agent.run_task(goal=user_message, context=context_from_output)\n",
        "                            final_reply = result_dict.get('final_output', '[Agent completed task but provided no final summary.]')\n",
        "                        elif box2_api_accessible:\n",
        "                            # Call Box 2 API\n",
        "                            payload = {\"task\": user_message, \"context\": context_from_output}\n",
        "                            api_response = requests.post(f\"{box2_api_url}/mcp/agent/action\", json=payload, timeout=120)\n",
        "                            if api_response.status_code == 200:\n",
        "                                result_dict = api_response.json()\n",
        "                                final_reply = result_dict.get('final_output', '[Agent completed task but provided no final summary via API.]')\n",
        "                            else:\n",
        "                                final_reply = f\"‚ùå API Error contacting Action Agent: {api_response.status_code} - {api_response.text}\"\n",
        "                        else:\n",
        "                            final_reply = \"‚ùå Neither direct session nor API access to Box 2 is available for the Action Agent.\"\n",
        "\n",
        "                        with chat_history_area:\n",
        "                            print() # Newline after \"Thinking...\"\n",
        "                            print(f\"[Assistant (Action Agent Team)]: {final_reply}\")\n",
        "                            print(\"-\" * 20)\n",
        "\n",
        "                    except Exception as e:\n",
        "                         with chat_history_area:\n",
        "                             print() # Newline after \"Thinking...\"\n",
        "                             print(f\"[System Error (Action Agent)]: {str(e)}\")\n",
        "\n",
        "                    chat_input.value = '' # Clear input after action agent call\n",
        "\n",
        "                else:\n",
        "                    # Fallback if direct chat is selected but API is not accessible\n",
        "                    with chat_history_area:\n",
        "                        print(f\"\\n‚ùå Selected agent '{selected_agent}' is not available in the current mode.\")\n",
        "                    chat_input.value = ''\n",
        "\n",
        "            except Exception as e:\n",
        "                 with chat_history_area:\n",
        "                     print(f\"\\nüí• ERROR sending chat message: {e}\")\n",
        "                 chat_input.value = '' # Clear input on general error\n",
        "\n",
        "\n",
        "        def on_chat_clear(b):\n",
        "            chat_history_area.clear_output()\n",
        "            chat_input.value = '' # Also clear the input field\n",
        "\n",
        "        # Assign event handlers\n",
        "        run_button.on_click(on_run_task)\n",
        "        tool_run_button.on_click(on_execute_tool)\n",
        "        clear_button.on_click(on_clear)\n",
        "        chat_send_button.on_click(on_chat_send)\n",
        "        chat_clear_button.on_click(on_chat_clear)\n",
        "\n",
        "        # --- Display Layout ---\n",
        "        ui_layout = widgets.VBox([\n",
        "            widgets.HTML(\"<h1>ü§ñ Unified Manus MCP System (Jupyter - Ollama)</h1>\"),\n",
        "            widgets.HTML(\"<h2>Agent Task Execution</h2>\"),\n",
        "            task_input,\n",
        "            context_input,\n",
        "            run_button,\n",
        "            widgets.HTML(\"<h2 style='margin-top: 20px;'>Tool Execution</h2>\"),\n",
        "            tool_name_dropdown,\n",
        "            tool_input_textarea,\n",
        "            tool_run_button,\n",
        "            widgets.HTML(\"<h2 style='margin-top: 20px;'>Direct Chat</h2>\"),\n",
        "            chat_agent_selector, # Add the agent selector dropdown\n",
        "            chat_history_area,\n",
        "            chat_input,\n",
        "            widgets.HBox([chat_send_button, chat_clear_button]),\n",
        "            widgets.HTML(\"<h2 style='margin-top: 20px;'>Output</h2>\"),\n",
        "            clear_button,\n",
        "            output_area\n",
        "        ])\n",
        "\n",
        "        display(ui_layout)\n",
        "\n",
        "    return create_jupyter_ui\n",
        "\n",
        "\n",
        "print(\"üåê Step 6: Setting up FastAPI server (Integrated or Proxy)...\")\n",
        "\n",
        "# --- Integrated or Proxy FastAPI App for Box 3 ---\n",
        "def create_integrated_or_proxy_server():\n",
        "    \"\"\"Create the FastAPI app for Box 3, either integrated or proxying to Box 2\"\"\"\n",
        "    app = FastAPI(\n",
        "        title=\"Unified Manus MCP Server - Box 3 (Ollama)\",\n",
        "        description=\"Integrated server with GUI interfaces and potential proxying to Ollama-powered Box 2\",\n",
        "        version=\"7.0.x\"\n",
        "    )\n",
        "\n",
        "    # CORS middleware\n",
        "    app.add_middleware(\n",
        "        CORSMiddleware,\n",
        "        allow_origins=[\"*\"], # Adjust for production\n",
        "        allow_credentials=True,\n",
        "        allow_methods=[\"*\"],\n",
        "        allow_headers=[\"*\"],\n",
        "    )\n",
        "\n",
        "    # Static files\n",
        "    site_dir = BASE_DIR / \"site\"\n",
        "    if site_dir.exists():\n",
        "        app.mount(\"/site\", StaticFiles(directory=str(site_dir)), name=\"site\")\n",
        "\n",
        "    # --- Root endpoint ---\n",
        "    @app.get(\"/\")\n",
        "    async def root():\n",
        "        \"\"\"Root endpoint\"\"\"\n",
        "        index_file = site_dir / \"index.html\"\n",
        "        if index_file.exists():\n",
        "            return FileResponse(str(index_file))\n",
        "        else:\n",
        "            return {\n",
        "                \"message\": \"Unified Manus MCP System - Box 3 (Ollama-powered)\",\n",
        "                \"version\": \"7.0.x\",\n",
        "                \"box2_status\": \"Integrated\" if box2_running_in_session else (\"Proxying\" if box2_api_accessible else \"Unavailable\"),\n",
        "                \"public_url\": public_url,\n",
        "                \"agent_session\": agent_session,\n",
        "                \"ollama_model\": ollama_model\n",
        "            }\n",
        "\n",
        "    # --- Health check ---\n",
        "    @app.get(\"/health\")\n",
        "    async def health():\n",
        "        \"\"\"Health check endpoint\"\"\"\n",
        "        box2_status = \"Unavailable\"\n",
        "        if box2_running_in_session:\n",
        "            box2_status = \"Integrated\"\n",
        "        elif box2_api_accessible:\n",
        "            try:\n",
        "                b2_response = requests.get(f\"{box2_api_url}/health\", timeout=2)\n",
        "                if b2_response.status_code == 200:\n",
        "                    b2_data = b2_response.json()\n",
        "                    if b2_data.get(\"status\") == \"healthy\":\n",
        "                        box2_status = \"Accessible (Healthy)\"\n",
        "                    else:\n",
        "                        box2_status = f\"Accessible (Degraded: {b2_data.get('status')})\"\n",
        "                else:\n",
        "                    box2_status = f\"Accessible (API Error: {b2_response.status_code})\"\n",
        "            except:\n",
        "                box2_status = \"Accessible (Health Check Failed)\"\n",
        "\n",
        "        return {\n",
        "            \"status\": \"healthy\" if (\"Healthy\" in box2_status or box2_status == \"Integrated\") else \"degraded\",\n",
        "            \"box\": 3,\n",
        "            \"version\": \"7.0.x\",\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"box2_status\": box2_status,\n",
        "            \"agent_session\": agent_session,\n",
        "            \"ollama_model\": ollama_model\n",
        "        }\n",
        "\n",
        "    # --- Proxy endpoints to Box 2 if it's running externally ---\n",
        "    if not box2_running_in_session and box2_api_accessible:\n",
        "        print(\"üîÑ Setting up proxy endpoints to external Box 2...\")\n",
        "\n",
        "        @app.api_route(\"/mcp/tools/call\", methods=[\"POST\"])\n",
        "        async def proxy_tool_call(request: Request):\n",
        "            try:\n",
        "                body = await request.body()\n",
        "                headers = dict(request.headers)\n",
        "                headers['Content-Type'] = 'application/json'\n",
        "                response = requests.post(f\"{box2_api_url}/mcp/tools/call\", data=body, headers=headers)\n",
        "                return Response(content=response.content, status_code=response.status_code, headers=dict(response.headers))\n",
        "            except Exception as e:\n",
        "                return JSONResponse(status_code=500, content={\"error\": f\"Proxy error: {str(e)}\"})\n",
        "\n",
        "        @app.api_route(\"/mcp/tools/list\", methods=[\"GET\"])\n",
        "        async def proxy_tool_list(request: Request):\n",
        "             try:\n",
        "                 params = dict(request.query_params)\n",
        "                 response = requests.get(f\"{box2_api_url}/mcp/tools/list\", params=params)\n",
        "                 return Response(content=response.content, status_code=response.status_code, headers=dict(response.headers))\n",
        "             except Exception as e:\n",
        "                 return JSONResponse(status_code=500, content={\"error\": f\"Proxy error: {str(e)}\"})\n",
        "\n",
        "        @app.api_route(\"/mcp/agent/action\", methods=[\"POST\"])\n",
        "        async def proxy_agent_action(request: Request):\n",
        "            try:\n",
        "                body = await request.body()\n",
        "                headers = dict(request.headers)\n",
        "                headers['Content-Type'] = 'application/json'\n",
        "                response = requests.post(f\"{box2_api_url}/mcp/agent/action\", data=body, headers=headers)\n",
        "                return Response(content=response.content, status_code=response.status_code, headers=dict(response.headers))\n",
        "            except Exception as e:\n",
        "                return JSONResponse(status_code=500, content={\"error\": f\"Proxy error: {str(e)}\"})\n",
        "\n",
        "        @app.api_route(\"/mcp/agent/memory\", methods=[\"GET\", \"POST\"])\n",
        "        async def proxy_agent_memory(request: Request):\n",
        "            try:\n",
        "                url = f\"{box2_api_url}/mcp/agent/memory\"\n",
        "                if request.method == \"POST\":\n",
        "                    url += \"/clear\"\n",
        "                body = await request.body() if request.method in [\"POST\", \"PUT\"] else None\n",
        "                headers = dict(request.headers)\n",
        "                headers['Content-Type'] = 'application/json'\n",
        "                if body:\n",
        "                    response = requests.request(request.method, url, data=body, headers=headers)\n",
        "                else:\n",
        "                     response = requests.request(request.method, url, headers=headers)\n",
        "                return Response(content=response.content, status_code=response.status_code, headers=dict(response.headers))\n",
        "            except Exception as e:\n",
        "                return JSONResponse(status_code=500, content={\"error\": f\"Proxy error: {str(e)}\"})\n",
        "\n",
        "        # --- Proxy the new Chat endpoint ---\n",
        "        @app.api_route(\"/mcp/chat\", methods=[\"POST\"])\n",
        "        async def proxy_chat(request: Request):\n",
        "            try:\n",
        "                body = await request.body()\n",
        "                headers = dict(request.headers)\n",
        "                headers['Content-Type'] = 'application/json'\n",
        "                # Use requests to forward the stream\n",
        "                # Note: True proxying of SSE streams is complex with standard requests.\n",
        "                # A more robust solution might use `httpx` or async libraries.\n",
        "                # For now, we proxy the request and return the response.\n",
        "                response = requests.post(f\"{box2_api_url}/mcp/chat\", data=body, headers=headers, stream=True)\n",
        "                # Returning a streaming response correctly requires more setup.\n",
        "                # This is a simplified proxy that might not handle streams perfectly.\n",
        "                # Consider using `httpx` StreamResponse for better SSE proxying.\n",
        "                return Response(content=response.content, status_code=response.status_code, headers=dict(response.headers))\n",
        "            except Exception as e:\n",
        "                 return JSONResponse(status_code=500, content={\"error\": f\"Chat Proxy error: {str(e)}\"})\n",
        "\n",
        "\n",
        "    elif box2_running_in_session:\n",
        "        print(\"üîó Box 2 is integrated, no proxy needed for its endpoints.\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Box 2 is not accessible, proxy endpoints will not function.\")\n",
        "\n",
        "    return app\n",
        "\n",
        "# Create the integrated/proxy app\n",
        "integrated_app = create_integrated_or_proxy_server()\n",
        "print(\"‚úÖ FastAPI server (Box 3) configured\")\n",
        "\n",
        "\n",
        "print(\"üíæ Step 7: Saving Box 3 configuration...\")\n",
        "\n",
        "def save_box3_state():\n",
        "    \"\"\"Save Box 3 state\"\"\"\n",
        "    state = {\n",
        "        \"interfaces_available\": [\"jupyter\", \"gradio\", \"api\"],\n",
        "        \"web_interface_ready\": True,\n",
        "        \"plugin_manifests_created\": True,\n",
        "        \"launch_modes\": [\"jupyter\", \"gradio\", \"api\", \"all\"],\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"box2_connection\": {\n",
        "            \"in_session\": box2_running_in_session,\n",
        "            \"api_accessible\": box2_api_accessible,\n",
        "            \"status\": \"Integrated\" if box2_running_in_session else (\"Accessible\" if box2_api_accessible else \"Unavailable\")\n",
        "        },\n",
        "        \"gradio_available\": GRADIO_AVAILABLE,\n",
        "        \"jupyter_available\": JUPYTER_AVAILABLE,\n",
        "        \"public_url\": public_url,\n",
        "        \"dashboard_url\": dashboard_url,\n",
        "        \"agent_session\": agent_session,\n",
        "        \"ollama_model\": ollama_model\n",
        "    }\n",
        "    config_file = BASE_DIR / \"config\" / \"box3_exports.json\"\n",
        "    config_file.parent.mkdir(parents=True, exist_ok=True) # Ensure config dir exists\n",
        "    try:\n",
        "        with open(config_file, \"w\") as f:\n",
        "            json.dump(state, f, indent=2)\n",
        "        print(f\"‚úÖ Box 3 state saved to {config_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to save Box 3 state: {e}\")\n",
        "\n",
        "save_box3_state()\n",
        "\n",
        "print(\"üîç Step 8: Final verification...\")\n",
        "\n",
        "def verify_box3_setup():\n",
        "    \"\"\"Verify Box 3 setup\"\"\"\n",
        "    checks = {\n",
        "        \"Web Interface\": (BASE_DIR / \"site\" / \"index.html\").exists(),\n",
        "        \"Plugin Manifests\": (BASE_DIR / \"site\" / \"ai-plugin.json\").exists(),\n",
        "        \"Configuration Files\": (BASE_DIR / \"config\" / \"box3_exports.json\").exists(),\n",
        "        \"Site Directory\": (BASE_DIR / \"site\").exists(),\n",
        "        \"Box 1 Config\": (BASE_DIR / \"config\" / \"box1_exports.json\").exists(),\n",
        "        \"Box 2 Config\": (BASE_DIR / \"config\" / \"box2_exports.json\").exists(),\n",
        "        \"Box 2 Connection\": box2_running_in_session or box2_api_accessible,\n",
        "        \"Gradio Availability\": not GRADIO_AVAILABLE or (GRADIO_AVAILABLE and setup_gradio_interface() is not None),\n",
        "        \"Jupyter Availability\": not JUPYTER_AVAILABLE or (JUPYTER_AVAILABLE and setup_jupyter_interface() is not None)\n",
        "    }\n",
        "    print(\"üîç Box 3 verification:\")\n",
        "    all_good = True\n",
        "    for check, status in checks.items():\n",
        "        status_icon = \"‚úÖ\" if status else \"‚ùå\"\n",
        "        print(f\" {status_icon} {check}: {'OK' if status else 'FAILED'}\")\n",
        "        if not status:\n",
        "            all_good = False\n",
        "    return all_good\n",
        "\n",
        "verification_passed = verify_box3_setup()\n",
        "\n",
        "if verification_passed:\n",
        "    print(\"üéâ BOX 3 SETUP COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"‚úÖ ALL SYSTEMS VERIFIED AND READY!\")\n",
        "    print(\"üöÄ LAUNCH OPTIONS:\")\n",
        "    print(\" ‚Ä¢ Jupyter Interface: launch_system('jupyter')\")\n",
        "    print(\" ‚Ä¢ Gradio Interface: launch_system('gradio')\")\n",
        "    print(\" ‚Ä¢ API Server Only: launch_system('api')\")\n",
        "    print(\" ‚Ä¢ ALL Interfaces: launch_system('all')\")\n",
        "    print(f\"üåç URLs:\")\n",
        "    print(f\" ‚Ä¢ Main API: {public_url}\")\n",
        "    print(f\" ‚Ä¢ Dashboard: {dashboard_url}\")\n",
        "    print(f\" ‚Ä¢ Web Interface: {public_url}/site/\")\n",
        "    print(f\" ‚Ä¢ API Docs: {public_url}/docs\")\n",
        "    print(f\"üìÅ System Directories:\")\n",
        "    print(f\" ‚Ä¢ Base: {BASE_DIR}\")\n",
        "    print(f\" ‚Ä¢ Workspace: {WORKSPACE_DIR}\")\n",
        "    print(f\" ‚Ä¢ Logs: {BASE_DIR / 'logs'}\")\n",
        "    print(f\" ‚Ä¢ Site: {BASE_DIR / 'site'}\")\n",
        "    print(f\"üîß System Status:\")\n",
        "    print(f\" ‚Ä¢ Environment: {'Google Colab' if IS_COLAB else 'Local'}\")\n",
        "    print(f\" ‚Ä¢ Box 2 Status: {'Integrated' if box2_running_in_session else ('Proxying' if box2_api_accessible else 'Unavailable')}\")\n",
        "    print(f\" ‚Ä¢ Ollama Model: {ollama_model}\")\n",
        "    print(f\" ‚Ä¢ Gradio Ready: {'Yes' if GRADIO_AVAILABLE else 'No'}\")\n",
        "    print(f\" ‚Ä¢ Jupyter Ready: {'Yes' if JUPYTER_AVAILABLE else 'No'}\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"üîÑ Box 3 is ready for launch!\")\n",
        "else:\n",
        "    print(\"‚ùå BOX 3 SETUP HAD ISSUES!\")\n",
        "    print(\"Please check the errors above.\")\n",
        "\n",
        "# --- Launch Functions ---\n",
        "def launch_system(mode: str = \"jupyter\"):\n",
        "    \"\"\"\n",
        "    Launch the system in different modes.\n",
        "    Modes: 'jupyter', 'gradio', 'api', 'all'\n",
        "    \"\"\"\n",
        "    global integrated_app # Use the app created earlier\n",
        "\n",
        "    if mode == \"jupyter\":\n",
        "        if JUPYTER_AVAILABLE:\n",
        "            print(\"üìì Launching Jupyter interface...\")\n",
        "            jupyter_ui = setup_jupyter_interface()\n",
        "            if jupyter_ui:\n",
        "                jupyter_ui()\n",
        "            else:\n",
        "                print(\"‚ùå Failed to setup Jupyter interface.\")\n",
        "        else:\n",
        "            print(\"‚ùå Jupyter is not available in this environment.\")\n",
        "\n",
        "    elif mode == \"gradio\":\n",
        "        if GRADIO_AVAILABLE:\n",
        "            print(\"üé® Launching Gradio interface...\")\n",
        "            demo = setup_gradio_interface()\n",
        "            if demo:\n",
        "                demo.launch(share=True, server_name=\"0.0.0.0\", server_port=7860)\n",
        "                print(f\"‚úÖ Gradio launched. Access at: http://localhost:7860 (or the public Gradio link if shared)\")\n",
        "            else:\n",
        "                print(\"‚ùå Failed to setup Gradio interface.\")\n",
        "        else:\n",
        "            print(\"‚ùå Gradio is not available.\")\n",
        "\n",
        "    elif mode == \"api\":\n",
        "        print(\"üåê Launching FastAPI server (Box 3)...\")\n",
        "        uvicorn.run(integrated_app, host=\"0.0.0.0\", port=8000)\n",
        "        print(f\"‚úÖ FastAPI server launched. Access at: {public_url}\")\n",
        "\n",
        "    elif mode == \"all\":\n",
        "        print(\"üîÑ Launching all interfaces...\")\n",
        "        def run_api():\n",
        "             uvicorn.run(integrated_app, host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "        api_thread = threading.Thread(target=run_api)\n",
        "        api_thread.daemon = True\n",
        "        api_thread.start()\n",
        "        print(f\"‚úÖ API Server started in background on port 8000\")\n",
        "\n",
        "        time.sleep(2)\n",
        "\n",
        "        if JUPYTER_AVAILABLE:\n",
        "            jupyter_ui = setup_jupyter_interface()\n",
        "            if jupyter_ui:\n",
        "                jupyter_thread = threading.Thread(target=jupyter_ui)\n",
        "                jupyter_thread.start()\n",
        "                print(\"‚úÖ Jupyter interface launched\")\n",
        "            else:\n",
        "                 print(\"‚ö†Ô∏è Jupyter interface setup failed\")\n",
        "\n",
        "        if GRADIO_AVAILABLE:\n",
        "            demo = setup_gradio_interface()\n",
        "            if demo:\n",
        "                def run_gradio():\n",
        "                    demo.launch(share=True, server_name=\"0.0.0.0\", server_port=7860, prevent_thread_lock=True)\n",
        "                    demo.block_thread()\n",
        "\n",
        "                gradio_thread = threading.Thread(target=run_gradio)\n",
        "                gradio_thread.daemon = True\n",
        "                gradio_thread.start()\n",
        "                print(\"‚úÖ Gradio interface launched\")\n",
        "                time.sleep(3)\n",
        "            else:\n",
        "                 print(\"‚ö†Ô∏è Gradio interface setup failed\")\n",
        "\n",
        "        print(\"üéâ All requested interfaces started!\")\n",
        "        print(f\"üåç API: {public_url}\")\n",
        "        print(f\"üìä Dashboard: {dashboard_url} (if applicable)\")\n",
        "        if GRADIO_AVAILABLE:\n",
        "            print(f\"üé® Gradio: Check output above or http://localhost:7860\")\n",
        "        print(\"‚ÑπÔ∏è Main thread will now idle. Stop with KeyboardInterrupt (Ctrl+C).\")\n",
        "\n",
        "        try:\n",
        "            while True:\n",
        "                time.sleep(1)\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"üõë Shutting down all services...\")\n",
        "\n",
        "    else:\n",
        "        print(f\"‚ùå Unknown mode: {mode}\")\n",
        "        print(\"Available modes: jupyter, gradio, api, all\")\n",
        "\n",
        "\n",
        "# Auto-launch Jupyter interface if in a Jupyter environment and script run directly\n",
        "if IS_COLAB or ('ipykernel' in sys.modules):\n",
        "    print(\"üìì Auto-launching Jupyter interface...\")\n",
        "    launch_system('jupyter')\n",
        "\n",
        "print(\"‚úÖ Box 3 initialization complete!\")\n",
        "print(\"üöÄ Use `launch_system('mode')` to start interfaces (e.g., `launch_system('gradio')`)\")\n",
        "\n",
        "# Export launch function\n",
        "__all__ = ['launch_system']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "00a0a1e23cfa4b9baa00fcb31f73d0a3",
            "fd79080c24504e7a9f6cde801c0c9765",
            "1bb58ff789d244019a519b120d93887e",
            "43a78a7fdde34ac791c41bd86d4a26d6",
            "f8380dbe79cd4b48b0c0fb2454fe11ec",
            "c639db22a553473590277cb8adc4d85b",
            "80dcc8fdf61f4572af76aeaf8333f909",
            "e571bf6fbbac4b3fa6dd86289fb41a00",
            "75859aafb1b549c996cd3159c3f26d0a",
            "95af9c4b58924957b27f3dc50bb65b58",
            "5287e3a02a374eb0bb88507f7c6f7146",
            "fad8251bf225499297e765ddab9e2572",
            "7d67b04f3d364e7aaa03ca7f8e8d9ae9",
            "a1b7418762cd40b7951224384804fd46",
            "fdc849fbbec94694b1df688876078ef5",
            "ec0567d2eed743a997bc53385dd81142",
            "1b7bdb38088f4a6e9a4535560b86516f",
            "032733b7681249449572d98c1943fae0",
            "d8f4a05c540349c084b45409ce084f56",
            "6543c26ba108449a898f019eadf401b8",
            "21f2467452b64b86a16479629e781eac",
            "43611cfb2b854fc784633b3b40487854",
            "af94abd8ea7e4891a89c320971edb961",
            "2e464d189a6148f5913e19f34e51208f",
            "bdc7f6514eff4126a81d281847bdb94d",
            "ae32dfdb2049405aaebb45e319b60f5d",
            "bedfc285d5db491293054f3f015f1fd3",
            "59cd4143a1ff4046b72dd94956a020a8",
            "a9de0bb3e0d1423a9cfdb78668f0ef2e",
            "b79449f1908545fc84de2fc50ebec2db",
            "d060348b63a649f5a296fa35e32f5acc",
            "0acb41ca7f8844aca78bb182bec4012b",
            "9e7dc1588ef546f089593282fd81157c",
            "50f6839c98e9441a802fe0e31015a51e",
            "2856181dc1ea41369c3bf6649eb51d49",
            "bcfa3af1dbf44d009f3a4846ad5dd242",
            "5520c7b163494cb781f355386563791b",
            "644fa0f200d44ef6b91a611b43bb431a",
            "0081009d33f6404683099c03b4858334",
            "5b0bf8ee88524afe8570e8ad467efe8b",
            "9300c3d417cd4876b6abf7a38f6269d2",
            "7ae586b030664dd08e55fb673e0231df",
            "214b53fc62a648eeabb080cc212fc6d9",
            "375aa19abba54e5fa9c6521eb2f18b33",
            "3481f530e6a54267b78739b3ed587ca7",
            "f96db7c694a24e88a6cc96b708fd26f7",
            "44869fb2dcbb4004b741c352e4e27e9a",
            "3b1b63f333d2468e9c632d38bc785890",
            "0403168a154b4833804778769876e32b",
            "1e79705f16e24606bc3e0eae3728e77d",
            "635c41cd3c7a4e3bb08dbabaf74e7b9d",
            "10565693042d4c2ebd1d67ecfb69d862",
            "b529adddef32449da2a9e3663121b362",
            "ea3da55775ca473db202f39570482abb",
            "748dffa064634fb5a53ed2035531a708",
            "947c1fc1d74748559722b86d57a4455b"
          ]
        },
        "id": "hTMGs8Ez-6pk",
        "outputId": "b749fea7-3a54-4148-f8f6-0c46db431724"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<h1>ü§ñ Unified Manus MCP System (Jupyter - Ollama)</h1>'), HTML(value='<h2>Agent Tas‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "00a0a1e23cfa4b9baa00fcb31f73d0a3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Box 3 initialization complete!\n",
            "üöÄ Use `launch_system('mode')` to start interfaces (e.g., `launch_system('gradio')`)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}